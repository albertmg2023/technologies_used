{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f950b746",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Bloque 7: Redes Neuronales Multicapa (MLPs)\n",
    "\n",
    "## ðŸ”¹ Â¿QuÃ© es un MLP?\n",
    "Un **PerceptrÃ³n Multicapa (MLP, *Multi-Layer Perceptron*)** es una red neuronal **feedforward**, es decir, la informaciÃ³n fluye siempre hacia adelante:\n",
    "\n",
    "âž¡ï¸ Entrada â†’ Capas ocultas â†’ Salida\n",
    "\n",
    "Cada neurona realiza tres pasos bÃ¡sicos:\n",
    "\n",
    "1. **CombinaciÃ³n lineal:**  \n",
    "   \\[\n",
    "   z = \\sum_{i=1}^n w_i x_i + b\n",
    "   \\]\n",
    "\n",
    "2. **FunciÃ³n de activaciÃ³n:**  \n",
    "   \\[\n",
    "   a = f(z)\n",
    "   \\]\n",
    "\n",
    "3. **PropagaciÃ³n hacia la siguiente capa** con el valor de activaciÃ³n \\(a\\).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Componentes principales de un MLP\n",
    "\n",
    "### 1. **Capas**\n",
    "- **Entrada:** recibe los datos originales (ejemplo: pÃ­xeles de MNIST).  \n",
    "- **Ocultas:** transforman los datos en representaciones mÃ¡s abstractas.  \n",
    "- **Salida:** genera la predicciÃ³n final (ejemplo: 10 neuronas para dÃ­gitos 0â€“9).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Funciones de activaciÃ³n**\n",
    "- **ReLU:**  \n",
    "  \\[\n",
    "  f(x) = \\max(0, x)\n",
    "  \\]  \n",
    "  âœ… RÃ¡pida, evita gradiente desapareciente, ideal para redes profundas.  \n",
    "\n",
    "- **Sigmoid:**  \n",
    "  \\[\n",
    "  f(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]  \n",
    "  âœ… Ãštil para probabilidades. âŒ Problema: saturaciÃ³n en valores extremos.  \n",
    "\n",
    "- **Tanh:**  \n",
    "  \\[\n",
    "  f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  \\]  \n",
    "  âœ… Salida entre -1 y 1, centrada en 0. âŒ Puede saturarse.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **InicializaciÃ³n de pesos**\n",
    "- **Xavier (Glorot):** mantiene la varianza de seÃ±ales estable â†’ bueno para *sigmoid* y *tanh*.  \n",
    "- **He (Kaiming):** ajustado para *ReLU*, compensa neuronas que se apagan.  \n",
    "\n",
    "ðŸ‘‰ Una buena inicializaciÃ³n evita gradientes explosivos o desaparecientes.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Optimizadores**\n",
    "- **SGD:** simple descenso de gradiente estocÃ¡stico. Lento pero estable.  \n",
    "- **Adam:** adapta el learning rate con momentos del gradiente. Muy usado.  \n",
    "- **RMSProp:** suaviza gradientes cuadrados, evita oscilaciones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. **RegularizaciÃ³n**\n",
    "- **Dropout:** apaga neuronas aleatoriamente durante el entrenamiento â†’ evita overfitting.  \n",
    "- **Batch Normalization:** normaliza activaciones dentro de cada minibatch â†’ entrenamiento mÃ¡s estable.  \n",
    "- **Gradient Clipping:** limita valores de gradientes para evitar explosiones.  \n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Learning Rate Scheduling**\n",
    "El *learning rate* controla la velocidad de aprendizaje.  \n",
    "\n",
    "- **StepLR:** reduce el learning rate cada cierto nÃºmero de Ã©pocas.  \n",
    "- **Cosine Annealing:** lo baja siguiendo una curva coseno.  \n",
    "- **ReduceLROnPlateau:** lo baja cuando la pÃ©rdida deja de mejorar.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Flujo completo de un MLP\n",
    "1. **Forward pass:** datos â†’ capas â†’ salida.  \n",
    "2. **CÃ¡lculo de pÃ©rdida:** compara predicciÃ³n vs. verdad.  \n",
    "3. **Backward pass:** cÃ¡lculo de gradientes (*backpropagation*).  \n",
    "4. **OptimizaciÃ³n:** ajuste de pesos con el optimizador.  \n",
    "5. **Entrenamiento por Ã©pocas:** la red mejora tras mÃºltiples pasadas por los datos.  \n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **En resumen:**  \n",
    "Un **MLP aprende representaciones jerÃ¡rquicas** de los datos mediante **transformaciones lineales + funciones de activaciÃ³n no lineales**.  \n",
    "Cada componente (activaciÃ³n, inicializaciÃ³n, optimizador, regularizaciÃ³n) influye en **quÃ© tan bien y rÃ¡pido aprende el modelo**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c326fe",
   "metadata": {},
   "source": [
    "**ImportaciÃ³n y ConfiguraciÃ³n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13841c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ================================================\n",
    "# Bloque 7: Redes Neuronales Multicapa (MLPs)\n",
    "# ================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Usar CPU si estÃ¡ disponible\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Usando:\", device)\n",
    "\n",
    "# TransformaciÃ³n de imÃ¡genes (28x28 -> tensor normalizado con media 0.5 y desv_standar 0.5)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Cargar MNIST\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Subconjunto pequeÃ±o para pruebas rÃ¡pidas\n",
    "# âš ï¸ Puedes quitarlo si tienes buena mÃ¡quina\n",
    "\n",
    "# ================================================\n",
    "# DataLoader en PyTorch\n",
    "# ================================================\n",
    "\n",
    "# train_dataset / test_dataset: datasets ya preparados (ej. MNIST)\n",
    "# torch.utils.data.Subset: selecciona un subconjunto de datos\n",
    "#   - range(10000) -> solo toma los primeros 10000 ejemplos del set de entrenamiento\n",
    "#   - range(2000)  -> solo toma los primeros 2000 ejemplos del set de prueba\n",
    "\n",
    "# DataLoader: crea un iterador que entrega los datos en **batches**\n",
    "#   - batch_size=64 -> cada iteraciÃ³n devuelve 64 ejemplos\n",
    "#   - shuffle=True  -> mezcla los datos cada Ã©poca (solo para entrenamiento)\n",
    "#   - shuffle=False -> no mezcla (Ãºtil para evaluaciÃ³n/testing)\n",
    "\n",
    "# Ejemplo de dimensiones:\n",
    "#   - train_loader iteraciÃ³n: x.shape = [64, 1, 28, 28], y.shape = [64]\n",
    "#   - Cada batch se pasa al modelo para entrenamiento o evaluaciÃ³n\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(10000))\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(2000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852720b",
   "metadata": {},
   "source": [
    "**DefiniciÃ³n del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo bÃ¡sico MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10, activation=\"relu\"):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Activaciones dinÃ¡micas\n",
    "        if activation == \"relu\":\n",
    "            act_fn = nn.ReLU()\n",
    "        elif activation == \"sigmoid\":\n",
    "            act_fn = nn.Sigmoid()\n",
    "        elif activation == \"tanh\":\n",
    "            act_fn = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"ActivaciÃ³n no soportada\")\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x.view(x.size(0), -1)\n",
    "        # ---------------------\n",
    "        # Esta lÃ­nea **aplana** cada imagen del batch en un vector 1D.\n",
    "        # x.size(0) = tamaÃ±o del batch (nÃºmero de ejemplos en la pasada actual)\n",
    "        # -1 = PyTorch calcula automÃ¡ticamente la otra dimensiÃ³n necesaria\n",
    "        # Ejemplo MNIST:\n",
    "        #   x original: [64, 1, 28, 28]  -> 64 imÃ¡genes, 1 canal, 28x28 pÃ­xeles\n",
    "        #   x despuÃ©s de view: [64, 784] -> cada imagen convertida en vector de 784 elementos\n",
    "        # Esto es necesario antes de pasar los datos a capas lineales (nn.Linear)\n",
    "        x = x.view(x.size(0), -1)  # Aplanar 28x28 â†’ 784\n",
    "        \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309458f5",
   "metadata": {},
   "source": [
    "**Funciones de entrenamiento y evaluaciÃ³n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61c014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            pred = y_pred.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846c1da",
   "metadata": {},
   "source": [
    "**Experimento con diferentes activaciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704cf538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Entrenando con activaciÃ³n: RELU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse CPUID\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.7454 | Test Acc: 0.8370\n",
      "Epoch 2 | Loss: 0.3439 | Test Acc: 0.8650\n",
      "Epoch 3 | Loss: 0.2745 | Test Acc: 0.8895\n",
      "\n",
      "ðŸ”¹ Entrenando con activaciÃ³n: SIGMOID\n",
      "Epoch 1 | Loss: 1.5557 | Test Acc: 0.7600\n",
      "Epoch 2 | Loss: 0.5617 | Test Acc: 0.8555\n",
      "Epoch 3 | Loss: 0.3577 | Test Acc: 0.8810\n",
      "\n",
      "ðŸ”¹ Entrenando con activaciÃ³n: TANH\n",
      "Epoch 1 | Loss: 0.6569 | Test Acc: 0.8765\n",
      "Epoch 2 | Loss: 0.2866 | Test Acc: 0.8905\n",
      "Epoch 3 | Loss: 0.2180 | Test Acc: 0.9055\n"
     ]
    }
   ],
   "source": [
    "for act in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "    print(f\"\\nðŸ”¹ Entrenando con activaciÃ³n: {act.upper()}\")\n",
    "    model = MLP(activation=act).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(3):  # pocas Ã©pocas para pruebas\n",
    "        loss = train(model, optimizer, criterion, train_loader)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3748a",
   "metadata": {},
   "source": [
    "**Probar diferentes optimizadores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84c28411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Entrenando con optimizador: SGD\n",
      "Epoch 1 | Loss: 2.2893 | Test Acc: 0.2025\n",
      "Epoch 2 | Loss: 2.2559 | Test Acc: 0.3160\n",
      "Epoch 3 | Loss: 2.2244 | Test Acc: 0.3550\n",
      "\n",
      "ðŸ”¹ Entrenando con optimizador: Adam\n",
      "Epoch 1 | Loss: 0.7327 | Test Acc: 0.8540\n",
      "Epoch 2 | Loss: 0.3328 | Test Acc: 0.8815\n",
      "Epoch 3 | Loss: 0.2663 | Test Acc: 0.8810\n",
      "\n",
      "ðŸ”¹ Entrenando con optimizador: RMSProp\n",
      "Epoch 1 | Loss: 0.7400 | Test Acc: 0.8160\n",
      "Epoch 2 | Loss: 0.3421 | Test Acc: 0.8220\n",
      "Epoch 3 | Loss: 0.2729 | Test Acc: 0.8860\n"
     ]
    }
   ],
   "source": [
    "optimizers = {\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"RMSProp\": optim.RMSprop\n",
    "}\n",
    "\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    print(f\"\\nðŸ”¹ Entrenando con optimizador: {opt_name}\")\n",
    "    model = MLP(activation=\"relu\").to(device)\n",
    "    optimizer = opt_class(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        loss = train(model, optimizer, criterion, train_loader)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d659f44",
   "metadata": {},
   "source": [
    "**RegularizaciÃ³n con Dropout y BatchNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c17140b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Entrenando MLP con Dropout + BatchNorm\n",
      "Epoch 1 | Loss: 0.6956 | Test Acc: 0.8790\n",
      "Epoch 2 | Loss: 0.3673 | Test Acc: 0.8825\n",
      "Epoch 3 | Loss: 0.3043 | Test Acc: 0.8985\n"
     ]
    }
   ],
   "source": [
    "class MLP_Reg(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        super(MLP_Reg, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # 50% de neuronas apagadas\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"\\nðŸ”¹ Entrenando MLP con Dropout + BatchNorm\")\n",
    "model = MLP_Reg().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train(model, optimizer, criterion, train_loader)\n",
    "    acc = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd4e48",
   "metadata": {},
   "source": [
    "**Scheduler y Gradient Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616c3746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Entrenando con Scheduler + Gradient Clipping\n",
      "Epoch 1 | Loss: 0.7311 | Test Acc: 0.8420 | LR: 0.01000\n",
      "Epoch 2 | Loss: 0.3263 | Test Acc: 0.8900 | LR: 0.00500\n",
      "Epoch 3 | Loss: 0.1960 | Test Acc: 0.8995 | LR: 0.00500\n",
      "Epoch 4 | Loss: 0.1659 | Test Acc: 0.8710 | LR: 0.00250\n",
      "Epoch 5 | Loss: 0.1250 | Test Acc: 0.9165 | LR: 0.00250\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”¹ Entrenando con Scheduler + Gradient Clipping\")\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    \n",
    "    acc = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Test Acc: {acc:.4f} | LR: {scheduler.get_last_lr()[0]:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
