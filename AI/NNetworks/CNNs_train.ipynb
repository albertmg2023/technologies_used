{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a9bced",
   "metadata": {},
   "source": [
    "## üîπ Concepto\n",
    "Una **Convolutional Neural Network (CNN)** es una red neuronal especializada en datos con estructura espacial, como im√°genes.  \n",
    "Las CNN aprenden autom√°ticamente **caracter√≠sticas jer√°rquicas** mediante:\n",
    "- **Convoluciones:** filtros que extraen patrones locales (bordes, texturas).  \n",
    "- **Pooling:** reducci√≥n de dimensionalidad y estabilidad frente a traslaciones.  \n",
    "- **Capas fully connected:** combinan las caracter√≠sticas para producir la salida final.\n",
    "\n",
    "En esta secci√≥n entrenaremos una **CNN desde cero** sobre el dataset **CIFAR-10** (im√°genes 32x32, 10 clases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b698c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# üîπ Importar librer√≠as y preparar dataset CIFAR-10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Transformaciones: normalizaci√≥n + aumento simple\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a2d93",
   "metadata": {},
   "source": [
    "## üîπ Definici√≥n de la CNN\n",
    "La arquitectura b√°sica incluye:\n",
    "- 2 capas convolucionales con ReLU + MaxPooling\n",
    "- Capa fully connected con Dropout para prevenir overfitting\n",
    "- Softmax impl√≠cito en CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff890b7",
   "metadata": {},
   "source": [
    "# üîπ Convolutional Neural Networks (CNNs) ‚Äì Explicaci√≥n completa\n",
    "\n",
    "Una **CNN (Convolutional Neural Network)** es un tipo de red neuronal especializada en datos con estructura de grilla, como im√°genes, y est√° dise√±ada para aprender **representaciones jer√°rquicas** mediante una combinaci√≥n de capas convolucionales, funciones de activaci√≥n, pooling, fully connected y t√©cnicas de regularizaci√≥n como Dropout y Batch Normalization. La idea principal es extraer autom√°ticamente caracter√≠sticas relevantes de las im√°genes sin necesidad de ingenier√≠a manual de features.\n",
    "\n",
    "Las **capas convolucionales** (`nn.Conv2d`) aplican filtros (kernels) sobre la imagen de entrada para capturar patrones locales como bordes, texturas o formas. Cada filtro genera un **mapa de activaci√≥n**, y la cantidad de filtros se define por el par√°metro `out_channels`. Los par√°metros principales de una convoluci√≥n son `in_channels` (canales de entrada, por ejemplo 3 para RGB), `out_channels` (n√∫mero de filtros), `kernel_size` (tama√±o del filtro, por ejemplo 3x3), `stride` (paso de desplazamiento del filtro) y `padding` (relleno de ceros alrededor de la imagen para mantener dimensiones). La salida de una convoluci√≥n es un tensor de forma `[batch_size, out_channels, H_out, W_out]`.\n",
    "\n",
    "Despu√©s de la convoluci√≥n se aplica una **funci√≥n de activaci√≥n** para introducir no linealidad en el modelo. Las m√°s comunes son: **ReLU**, que toma el m√°ximo entre cero y la entrada, evitando gradientes desaparecientes; **Sigmoid**, √∫til para probabilidades, pero que puede saturar en valores extremos; y **Tanh**, que centra la salida entre -1 y 1, aunque tambi√©n puede saturar los gradientes.\n",
    "\n",
    "Luego se suelen aplicar capas de **pooling**, generalmente `MaxPool2d`, que reducen las dimensiones espaciales del mapa de activaciones conservando la informaci√≥n m√°s relevante. Esto disminuye la complejidad del modelo, hace que la red sea m√°s resistente a traslaciones y reduce la cantidad de par√°metros en las capas posteriores. Por ejemplo, un `MaxPool2d(2)` reduce la altura y anchura a la mitad tomando el valor m√°ximo de cada bloque 2x2.\n",
    "\n",
    "Despu√©s de varias capas de convoluci√≥n + activaci√≥n + pooling, los mapas de activaci√≥n se **aplanan** (`x.view(x.size(0), -1)`) para ser procesados por capas **fully connected** (`nn.Linear`). Estas capas conectan todas las neuronas de la entrada con todas las neuronas de la salida, combinando las caracter√≠sticas extra√≠das por las convoluciones para generar la predicci√≥n final. Los par√°metros de `nn.Linear` son `in_features` (n√∫mero de entradas, normalmente la salida aplanada) y `out_features` (n√∫mero de salidas, por ejemplo el n√∫mero de clases).\n",
    "\n",
    "Para mejorar la **generalizaci√≥n y evitar overfitting**, se suele usar **Dropout** (`nn.Dropout(p)`), que apaga aleatoriamente un porcentaje `p` de neuronas durante el entrenamiento. Esto fuerza a la red a no depender excesivamente de ciertas neuronas y distribuye la informaci√≥n entre todas.\n",
    "\n",
    "\n",
    "# üîπ Explicaci√≥n detallada de cada capa\n",
    "\n",
    "### 1Ô∏è‚É£ nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "- **Prop√≥sito:** capa convolucional que extrae caracter√≠sticas locales de la imagen.\n",
    "- **Par√°metros:**\n",
    "  - `3`: n√∫mero de canales de entrada. Ejemplo: 3 para im√°genes RGB.\n",
    "  - `32`: n√∫mero de filtros (o mapas de activaci√≥n) que se aplicar√°n, es decir, cu√°ntas caracter√≠sticas distintas extraer√° esta capa.\n",
    "  - `kernel_size=3`: tama√±o del filtro (3x3). Este filtro se mueve sobre la imagen aplicando un producto punto local.\n",
    "  - `padding=1`: agrega un borde de ceros alrededor de la imagen para mantener las dimensiones (H y W) iguales despu√©s de la convoluci√≥n.\n",
    "\n",
    "- **Salida:** si la imagen de entrada era 32x32x3, la salida ser√° 32x32x32 (altura x anchura x filtros).\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ nn.ReLU()\n",
    "- **Prop√≥sito:** funci√≥n de activaci√≥n no lineal.\n",
    "- **Qu√© hace:** reemplaza todos los valores negativos por 0.  \n",
    "- **Por qu√© se usa:** introduce no linealidad y evita el problema de gradientes desaparecidos en redes profundas.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ nn.MaxPool2d(2)\n",
    "- **Prop√≥sito:** capa de *pooling* para reducir las dimensiones espaciales.\n",
    "- **Par√°metros:**\n",
    "  - `2`: tama√±o de la ventana de pooling (2x2).  \n",
    "- **Qu√© hace:** divide la entrada en bloques 2x2 y toma el valor m√°ximo de cada bloque.\n",
    "- **Efecto:** reduce la altura y anchura a la mitad (por ejemplo, de 32x32 a 16x16), manteniendo la profundidad (n√∫mero de filtros).\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "- **Prop√≥sito:** segunda capa convolucional que extrae caracter√≠sticas m√°s complejas.\n",
    "- **Par√°metros:**\n",
    "  - `32`: canales de entrada (provenientes de la salida de la capa anterior).\n",
    "  - `64`: n√∫mero de filtros, aumentando la capacidad de aprender caracter√≠sticas m√°s abstractas.\n",
    "  - `kernel_size=3`: tama√±o del filtro (3x3).\n",
    "  - `padding=1`: mantiene la dimensi√≥n espacial igual.\n",
    "\n",
    "- **Salida:** si la entrada era 16x16x32, la salida ser√° 16x16x64.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ nn.ReLU()\n",
    "- Igual que la primera ReLU, aplica no linealidad.\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ nn.MaxPool2d(2)\n",
    "- Igual que la primera capa de pooling.\n",
    "- Reduce las dimensiones de 16x16 a 8x8, manteniendo los 64 mapas de activaci√≥n.\n",
    "\n",
    "\n",
    "El **flujo completo** de una CNN t√≠pica para im√°genes 32x32x3 (como CIFAR-10) ser√≠a:\n",
    "\n",
    "1. Entrada: Imagen 32x32x3.\n",
    "2. Conv2d(3‚Üí32, 3x3) + ReLU ‚Üí extracci√≥n de caracter√≠sticas locales.\n",
    "3. MaxPool2d(2) ‚Üí reducci√≥n de dimensi√≥n espacial a 16x16x32.\n",
    "4. Conv2d(32‚Üí64, 3x3) + ReLU ‚Üí extracci√≥n de caracter√≠sticas m√°s complejas.\n",
    "5. MaxPool2d(2) ‚Üí reducci√≥n a 8x8x64.\n",
    "6. Conv2d(64‚Üí128, 3x3) + ReLU ‚Üí caracter√≠sticas de alto nivel.\n",
    "7. MaxPool2d(2) ‚Üí reducci√≥n a 4x4x128.\n",
    "8. Flatten ‚Üí tensor de 2048 neuronas (4*4*128).\n",
    "9. Linear(2048‚Üí256) + ReLU ‚Üí combinaci√≥n de caracter√≠sticas.\n",
    "10. Dropout(p=0.5) ‚Üí regularizaci√≥n.\n",
    "11. Linear(256‚Üí10) ‚Üí salida final de clases.\n",
    "\n",
    "Durante el **forward pass**, la imagen pasa por todas estas capas generando activaciones que representan cada vez niveles m√°s abstractos de la informaci√≥n. Luego se calcula la **p√©rdida** comparando la predicci√≥n con la verdad (por ejemplo, `CrossEntropyLoss`). En el **backward pass**, se calculan los gradientes mediante **backpropagation**, y un **optimizador** (Adam, SGD, RMSProp) actualiza los pesos. El entrenamiento se realiza por **√©pocas**, mejorando iterativamente la capacidad de la red para generalizar.\n",
    "\n",
    "Opcionalmente, se puede usar **Batch Normalization** para normalizar las activaciones dentro de cada mini-batch, estabilizando y acelerando el entrenamiento. Tambi√©n se puede aplicar **gradient clipping** para limitar el tama√±o de los gradientes y evitar explosiones.\n",
    "\n",
    "Las CNNs son muy efectivas en **clasificaci√≥n, detecci√≥n de objetos, segmentaci√≥n de im√°genes y reconocimiento de patrones**. Adem√°s, se pueden usar en **Transfer Learning**, aprovechando modelos preentrenados como ResNet o VGG y reemplazando solo la √∫ltima capa para adaptarlos a un nuevo dataset, reduciendo tiempos de entrenamiento y mejorando performance.\n",
    "\n",
    "En resumen, una CNN combina convoluciones, activaciones, pooling, fully connected y regularizaci√≥n para aprender representaciones jer√°rquicas de im√°genes de manera eficiente, y cada componente influye en la capacidad de aprendizaje, velocidad de entrenamiento y generalizaci√≥n del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62538f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64*8*8, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e143b6",
   "metadata": {},
   "source": [
    "## üîπ Entrenamiento de la CNN\n",
    "Usamos **Adam** como optimizador y **CrossEntropyLoss** para clasificaci√≥n m√∫ltiple.  \n",
    "Entrenaremos por unas pocas √©pocas para demostraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c676ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.1488 | Accuracy: 0.5928\n",
      "Epoch 2 | Loss: 1.1182 | Accuracy: 0.6062\n",
      "Epoch 3 | Loss: 1.0852 | Accuracy: 0.6189\n",
      "Epoch 4 | Loss: 1.0630 | Accuracy: 0.6255\n",
      "Epoch 5 | Loss: 1.0428 | Accuracy: 0.6320\n",
      "Epoch 6 | Loss: 1.0285 | Accuracy: 0.6372\n",
      "Epoch 7 | Loss: 1.0089 | Accuracy: 0.6456\n",
      "Epoch 8 | Loss: 1.0002 | Accuracy: 0.6508\n",
      "Epoch 9 | Loss: 0.9865 | Accuracy: 0.6576\n",
      "Epoch 10 | Loss: 0.9761 | Accuracy: 0.6585\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = y_pred.max(1)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "train(cnn_model, train_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e49a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
