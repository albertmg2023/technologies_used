{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4529721e",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Concepto de Stacking con Redes Neuronales (NN)\n",
    "\n",
    "**Stacking:**\n",
    "\n",
    "- Entrenas varias redes base \\(M_1, M_2, M_3, \\dots\\) sobre tu dataset.\n",
    "- Cada red produce una predicci√≥n \\(y_i\\) (por ejemplo, **logits** o **probabilidades**).\n",
    "- Estas predicciones se usan como **features** para un modelo final llamado **meta-modelo**.\n",
    "- El **meta-modelo** aprende los **pesos √≥ptimos** para combinar las salidas de las redes base.\n",
    "\n",
    "üìå **Ventaja:**\n",
    "\n",
    "- Permite que la combinaci√≥n final sea **no lineal**, mucho m√°s flexible que un simple promedio o votaci√≥n.\n",
    "- Se puede usar otra **red neuronal peque√±a** como meta-modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb939e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: cpu\n",
      "Entrenando BaseNN 1...\n",
      "Accuracy BaseNN 1: 0.9610\n",
      "Entrenando BaseNN 2...\n",
      "Accuracy BaseNN 2: 0.9602\n",
      "Entrenando BaseNN 3...\n",
      "Accuracy BaseNN 3: 0.9614\n",
      "Meta-model Epoch 1 | Loss: 0.1359\n",
      "Meta-model Epoch 2 | Loss: 0.0109\n",
      "Meta-model Epoch 3 | Loss: 0.0093\n",
      "Meta-model Epoch 4 | Loss: 0.0071\n",
      "Meta-model Epoch 5 | Loss: 0.0058\n",
      "Meta-model Epoch 6 | Loss: 0.0061\n",
      "Meta-model Epoch 7 | Loss: 0.0059\n",
      "Meta-model Epoch 8 | Loss: 0.0054\n",
      "Meta-model Epoch 9 | Loss: 0.0059\n",
      "Meta-model Epoch 10 | Loss: 0.0044\n",
      "\n",
      "‚úÖ Accuracy final del meta-modelo: 0.964\n",
      "Accuracy de cada BaseNN: [0.961, 0.9602, 0.9614]\n"
     ]
    }
   ],
   "source": [
    "# üìò Notebook: Stacking MLPs optimizado para MNIST\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# ================================================\n",
    "# 1Ô∏è‚É£ Preparaci√≥n de datos MNIST\n",
    "# ================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Para demo, usamos un subset peque√±o (puedes usar el dataset completo)\n",
    "train_dataset = Subset(train_dataset, range(20000))  \n",
    "test_dataset = Subset(test_dataset, range(5000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Usando:\", device)\n",
    "\n",
    "# ================================================\n",
    "# 2Ô∏è‚É£ Definici√≥n de redes base\n",
    "# ================================================\n",
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, output_size=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten 28x28 ‚Üí 784\n",
    "        return self.net(x)\n",
    "\n",
    "# ================================================\n",
    "# 3Ô∏è‚É£ Entrenamiento de cada base y evaluaci√≥n\n",
    "# ================================================\n",
    "def train_base(model, epochs=10):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            correct += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "# Crear y entrenar 3 redes base\n",
    "base_models = [BaseNN().to(device) for _ in range(3)]\n",
    "base_accs = []\n",
    "\n",
    "for i, model in enumerate(base_models):\n",
    "    print(f\"Entrenando BaseNN {i+1}...\")\n",
    "    acc = train_base(model)\n",
    "    base_accs.append(acc)\n",
    "    print(f\"Accuracy BaseNN {i+1}: {acc:.4f}\")\n",
    "\n",
    "# ================================================\n",
    "# 4Ô∏è‚É£ Construcci√≥n de dataset para meta-modelo\n",
    "# ================================================\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Funci√≥n: get_logits\n",
    "# =========================================================\n",
    "# Esta funci√≥n tiene como objetivo calcular las salidas \"logits\" de un conjunto \n",
    "# de modelos base sobre un conjunto de datos dado. Es fundamental en t√©cnicas \n",
    "# de ensemble tipo \"stacking\", donde usamos las predicciones de modelos base \n",
    "# como caracter√≠sticas de entrada para un meta-modelo.\n",
    "#\n",
    "# Conceptos clave:\n",
    "# 1Ô∏è‚É£ Logits:\n",
    "#    - Los logits son las salidas directas de la √∫ltima capa lineal de una red \n",
    "#      neuronal antes de aplicar cualquier funci√≥n de activaci√≥n como softmax. \n",
    "#    - Por ejemplo, en clasificaci√≥n de 10 clases (MNIST), un modelo devuelve un \n",
    "#      vector de 10 n√∫meros reales por cada muestra. Cada n√∫mero representa \n",
    "#      la \"evidencia\" o \"puntaje sin normalizar\" para esa clase.\n",
    "#    - El meta-modelo generalmente aprende a combinar estas evidencias de varios \n",
    "#      modelos base para mejorar la predicci√≥n final.\n",
    "#\n",
    "# 2Ô∏è‚É£ detach():\n",
    "#    - `.detach()` crea un nuevo tensor que comparte los datos con el original, \n",
    "#      pero **corta el grafo de autograd**. Es decir, PyTorch ya no rastrear√° \n",
    "#      operaciones posteriores para calcular gradientes respecto a este tensor.\n",
    "#    - Esto es cr√≠tico aqu√≠ porque:\n",
    "#      * No queremos que el c√°lculo de la p√©rdida del meta-modelo retropropague \n",
    "#        gradientes hacia los modelos base, ya que esos modelos ya est√°n entrenados.\n",
    "#      * Si no usamos detach(), PyTorch intentar√≠a construir un grafo enorme \n",
    "#        combinando el meta-modelo y los modelos base, aumentando memoria y riesgo \n",
    "#        de errores de gradientes.\n",
    "#      * Adem√°s, se gastar√≠an recursos innecesarios recalculando gradientes que \n",
    "#        no necesitamos, ralentizando el entrenamiento.\n",
    "#\n",
    "# Flujo detallado de la funci√≥n:\n",
    "# -----------------------------------------\n",
    "# 1. Inicializa dos listas vac√≠as:\n",
    "#    - all_logits ‚Üí para almacenar los logits concatenados de todos los modelos\n",
    "#    - all_labels ‚Üí para almacenar las etiquetas verdaderas correspondientes\n",
    "#\n",
    "# 2. Itera por cada batch del DataLoader:\n",
    "#    a) x, y = batch\n",
    "#       - x: features de entrada del batch\n",
    "#       - y: etiquetas verdaderas del batch\n",
    "#    b) x = x.to(device)\n",
    "#       - Mueve los datos al mismo dispositivo que los modelos (CPU o GPU)\n",
    "#    c) logits = [m(x).detach() for m in models]\n",
    "#       - Calcula la salida de cada modelo base sobre x\n",
    "#       - Se usa detach() para romper la conexi√≥n con los grafos de los modelos base\n",
    "#       - Esto devuelve los logits como tensores que **no participan en retropropagaci√≥n**\n",
    "#    d) stacked_logits = torch.cat(logits, dim=1)\n",
    "#       - Concatenamos horizontalmente los logits de todos los modelos\n",
    "#       - Si hay 3 modelos y cada uno devuelve 10 logits, stacked_logits tendr√° 30 \n",
    "#         columnas por muestra\n",
    "#    e) all_logits.append(stacked_logits.cpu())\n",
    "#       - Movemos los logits a CPU para reducir uso de GPU y los almacenamos\n",
    "#    f) all_labels.append(y)\n",
    "#       - Guardamos las etiquetas originales para entrenar/evaluar el meta-modelo\n",
    "#\n",
    "# 3. Al final, usamos torch.cat() sobre los batches para devolver un tensor completo:\n",
    "#    - all_logits ‚Üí tensor de tama√±o (num_muestras, suma_de_logits_por_modelo)\n",
    "#    - all_labels ‚Üí tensor de tama√±o (num_muestras,)\n",
    "#\n",
    "# Por qu√© es √∫til y c√≥mo se implementa en stacking:\n",
    "# -----------------------------------------\n",
    "# - Cada modelo base puede aprender patrones distintos en los datos.\n",
    "# - El meta-modelo toma como entrada todos los logits y aprende a ponderarlos \n",
    "#   para mejorar la predicci√≥n final.\n",
    "# - Gracias a detach(), los pesos de los modelos base permanecen fijos; s√≥lo se \n",
    "#   entrena el meta-modelo.\n",
    "# - Esta implementaci√≥n es eficiente: procesamos batches completos, movemos \n",
    "#   datos a CPU para almacenamiento y evitamos construir grafos innecesarios.\n",
    "#\n",
    "# Ejemplo de uso:\n",
    "# meta_X_train, meta_y_train = get_logits(base_models, train_loader)\n",
    "# meta_X_test, meta_y_test = get_logits(base_models, test_loader)\n",
    "# - meta_X_train / meta_X_test ‚Üí entrada para el meta-modelo\n",
    "# - meta_y_train / meta_y_test ‚Üí etiquetas verdaderas para entrenamiento/evaluaci√≥n\n",
    "\n",
    "\n",
    "def get_logits(models, loader):\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = [m(x).detach() for m in models]  # <-- Aqu√≠ usamos detach()\n",
    "        stacked_logits = torch.cat(logits, dim=1)\n",
    "        all_logits.append(stacked_logits.cpu())\n",
    "        all_labels.append(y)\n",
    "    return torch.cat(all_logits), torch.cat(all_labels)\n",
    "\n",
    "meta_X_train, meta_y_train = get_logits(base_models, train_loader)\n",
    "meta_X_test, meta_y_test = get_logits(base_models, test_loader)\n",
    "\n",
    "# ================================================\n",
    "# 5Ô∏è‚É£ Definici√≥n y entrenamiento del meta-modelo\n",
    "# ================================================\n",
    "class MetaNN(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_size=128, output_size=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "meta_model = MetaNN().to(device)\n",
    "optimizer = optim.Adam(meta_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dataset PyTorch para meta-modelo\n",
    "meta_train_loader = DataLoader(list(zip(meta_X_train, meta_y_train)), batch_size=128, shuffle=True)\n",
    "meta_test_loader = DataLoader(list(zip(meta_X_test, meta_y_test)), batch_size=128, shuffle=False)\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(10):\n",
    "    meta_model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in meta_train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = meta_model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(meta_model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Meta-model Epoch {epoch+1} | Loss: {total_loss/len(meta_train_loader):.4f}\")\n",
    "\n",
    "# Evaluaci√≥n meta-modelo\n",
    "meta_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in meta_test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = meta_model(x)\n",
    "        correct += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "meta_acc = correct / total\n",
    "\n",
    "print(\"\\n‚úÖ Accuracy final del meta-modelo:\", round(meta_acc,4))\n",
    "print(\"Accuracy de cada BaseNN:\", [round(a,4) for a in base_accs])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
